################################################################################
#Author  : Sharath P Dandamudi
#Date    : 07JAN2016
#Purpose : Text classification using Naive Bayes algorithm
#Version : v1.0
#Note    : Install the packages in library(" ") before executing the codes
################################################################################

#Clearing R console
rm(list=ls())

#Set the working directory
setwd("C:/Users/Sharath P Dandamudi/Desktop/TM")

#Import dataset
atac_wc <- read.csv("atac_new.csv", stringsAsFactors = FALSE)

#View dataset
View(atac_wc)

#Check frequency distribution of classes in dataset
table(atac_wc$type)

#Initiating the 'tm' library
library(tm)

#Converting the dependent variable to a categorical variable
atac_wc$type <- factor(atac_wc$type)

#Creating corpus
atac_corpus <- Corpus(VectorSource(atac_wc$content))

#---Cleaning text data---#

#Converting all text to lower-case
corpus_clean <- tm_map(atac_corpus, content_transformer(tolower))

#Removing stopwords in English
corpus_clean <- tm_map(corpus_clean, removeWords, c(stopwords('english')))

#Removing numbers
corpus_clean <- tm_map(corpus_clean, removeNumbers)

#Removing punctuation
corpus_clean <- tm_map(corpus_clean, removePunctuation)

#Removing special characters - all non alphanumeric
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
corpus_clean <- tm_map(corpus_clean, removeSpecialChars)

#Importing custom stop words files
stop1 = read.table("stop1.txt", header = TRUE)
stop2 = read.table("stop2.txt", header = TRUE)
stop3 = read.table("stop3.txt", header = TRUE)
stop4 = read.table("stop4.txt", header = TRUE)
stop5 = read.table("stop5.txt", header = TRUE)

#Storing the stop word files into vectors
stop_vec1 = as.vector(stop1$stop)
stop_vec2 = as.vector(stop2$stop)
stop_vec3 = as.vector(stop3$stop)
stop_vec4 = as.vector(stop4$stop)
stop_vec5 = as.vector(stop5$stop)


#Storing the stop word files into vectors
corpus_clean <- tm_map(corpus_clean, removeWords, c(stop_vec1,stopwords('english')))
corpus_clean <- tm_map(corpus_clean, removeWords, c(stop_vec2))
corpus_clean <- tm_map(corpus_clean, removeWords, c(stop_vec3)) 
corpus_clean <- tm_map(corpus_clean, removeWords, c(stop_vec4)) 
corpus_clean <- tm_map(corpus_clean, removeWords, c(stop_vec5,"analyze","analyse","analysis","analyses"))

#Stripping white spaces
corpus_clean <- tm_map(corpus_clean, stripWhitespace)

#Stemming the document
corpus_clean <- tm_map(corpus_clean, stemDocument)

#Removing stop words again after stemming
corpus_clean <- tm_map(corpus_clean, removeWords, c("data","question","work","analytical","analysis"))

inspect(atac_corpus[1:3])

inspect(corpus_clean[1:3])

#Converting text into plain text document
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)

#Creating a word cloud
wordcloud(corpus_clean, min.freq = 10, random.order = FALSE)

#Initiating library RWeka
library(RWeka)

#dtm <- DocumentTermMatrix(corpus_clean)

#Creating 1 and 2 grams and creating a sparse matrix
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
dtm <- DocumentTermMatrix(corpus_clean, control = list(tokenize = BigramTokenizer))

#Computing frequency column wise to get the terms with highest frequency
freq <- colSums(as.matrix(dtm))

#Ordering the frequency of terms
ord <- order(freq)
ord

#Check most and least frequent words
freq[head(ord)]
freq[tail(ord)]

#Export the matrix (to see how) it looks to an excel file
m <- as.matrix(dtm)
write.csv(m, file = 'matrix.csv')


head(table(freq),20)
tail(table(freq),20)

#Splitting raw data frame
atac_raw_train <- atac_wc[1:375, ]
atac_raw_test <- atac_wc[376:536, ]

#Splitting document term matrix
atac_dtm_train <- dtm[1:375, ]
atac_dtm_test <- dtm[376:536, ]

#Splitting corpus
atac_corpus_train <- corpus_clean[1:375]
atac_corpus_test <- corpus_clean[376:536]

#compare the proportion of type in the training and test data frames
prop.table(table(atac_raw_train$type))
prop.table(table(atac_raw_test$type))

#Save list of frequent terms for use later
atac_dict <- c(findFreqTerms(atac_dtm_train, 2))

#To limit our training and test matrixes to only the words in the preceding dictionary
atac_train <- DocumentTermMatrix(atac_corpus_train,list(dictionary = atac_dict))
atac_test <- DocumentTermMatrix(atac_corpus_test,list(dictionary = atac_dict))


#The naive Bayes classifier is typically trained on data with categorical features. This
#poses a problem since the cells in the sparse matrix indicate a count of the times a
#word appears in a message. We should change this to a factor variable that simply
#indicates yes or no depending on whether the word appears at all
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0)
  x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
  return (x)
  }

#Create factor indicating Yes or No for whether each column's word appears in the messages comprising the rows
atac_train <- apply(atac_train, MARGIN = 2, convert_counts)
atac_test <- apply(atac_test, MARGIN = 2, convert_counts)

#Initiating e0171 library to run a Naive Bayes model
library(e1071)

#Running a Naive Bayes model and later scoring the testing data (Iter - 1)
atac_classifier <- naiveBayes(atac_train, atac_raw_train$type)
atac_test_pred <- predict(atac_classifier, atac_test)

library(gmodels)
#Initiating gmodels library to run a Mis-classification table
CrossTable(atac_test_pred, atac_raw_test$type,prop.chisq = FALSE, prop.t = FALSE,
           dnn = c('predicted', 'actual'))


#Running a Naive Bayes model and later scoring the testing data with laplace correction (Iter - 2)
atac_classifier2 <- naiveBayes(atac_train, atac_raw_train$type,laplace = 1)
atac_test_pred2 <- predict(atac_classifier2, atac_test)


CrossTable(atac_test_pred2, atac_raw_test$type,prop.chisq = FALSE, prop.t = FALSE,
           dnn = c('predicted', 'actual'))



